import streamlit as st
from groq import Groq
import os
from dotenv import load_dotenv

# 1. Load environment variables
load_dotenv()

# 2. Setup Page Config
st.set_page_config(page_title="AI9Campus Smart Tutor", page_icon=":robot_face:", layout="centered")
st.title("SchoolAI Bot generated by AI9Campus")
st.caption("One AI Platform for Students, Teachers & Administrators")

# 3. Initialize Groq Client
# Make sure your .env file has "GROK-API-KEY" inside it!
api_key = os.getenv("GROK-API-KEY")
if not api_key:
    st.error("API Key not found! Please check your .env file.")
    st.stop()

client = Groq(api_key=api_key)

# 4. Initialize Chat History
if 'message' not in st.session_state:
    st.session_state['message'] = []

# 5. Display Previous Messages
for msg in st.session_state.message:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# 6. Handle User Input
prompt = st.chat_input("Enter your message here...")

# Add a "System" instruction at the very start of the chat history
if len(st.session_state.message) == 0:
    st.session_state.message.append({
        "role": "system", 
        "content": "You are a friendly and strict school tutor for students in India. "
                   "You explain concepts simply using examples and real life analogies very easily from the SSC/Telangana curriculum. "
                   "Do not answer non-educational questions."
    })
    
if prompt:
    # Add user message to history
    st.session_state.message.append({"role": "user", "content": prompt})
    
    # Display user message
    with st.chat_message("user"):
        st.markdown(prompt)

    # Display assistant response
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        try:
            # Create the streaming request
            stream = client.chat.completions.create(
                model="moonshotai/kimi-k2-instruct-0905",  # Ensure this model name is correct for Groq
                messages=st.session_state.message,
                max_completion_tokens=2048,
                temperature=0.7,
                stream=True
            )

            # Process the stream
            for chunk in stream:
                # Check if content exists in this chunk
                if (chunk.choices 
                    and chunk.choices[0].delta 
                    and chunk.choices[0].delta.content):
                    
                    token = chunk.choices[0].delta.content
                    full_response += token
                    placeholder.markdown(full_response)
            
            # Save the final response to history
            if full_response:
                st.session_state.message.append({"role": "assistant", "content": full_response})
            else:
                st.warning("The model returned an empty response.")

        except Exception as e:
            st.error(f"An error occurred: {e}")